{
 "metadata": {
  "name": "",
  "signature": "sha256:b85238c2e667c90464bd7a36ff4e29287c2255b6c65983ea76cf2f71af256990"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The Vector Space Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook introduces the simplest form of the vector space model of terms and documents. We use this model to extend our inverted index (defined in a previous notebook) to support ranked query results. We'll see how this more useful than just returning the intersection of the postings list.\n",
      "\n",
      "The vector model we'll use in this notebook is an example of a _sparse representation_. Sparseness is often desirable, but we'll discuss how in this particular case, representing words with sparse vectors has weaknesses."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from collections import defaultdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll start by defining a few functions that should be familiar from the inverted indices notebook: `build_lexicon` and `build_inverted_index`. These are identical to the definitions from previous classes.\n",
      "\n",
      "One interesting thing to note is that the `sort` function is only available if you have imported numpy. The default Python function you want is `sorted`. If you use `sort`, you get back `numpy.ndarray`, which is fine as long as you're aware of what's going on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_lexicon(docs):\n",
      "    '''\n",
      "    Given a list of strings representing representing a collection of documents, \n",
      "    return a sorted list of the terms that appear in the documents\n",
      "    '''\n",
      "    lexicon = []\n",
      "    for d in docs:\n",
      "        for t in d.split():\n",
      "            if t not in lexicon:\n",
      "                lexicon.append(t)\n",
      "    return sort(lexicon)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_inverted_index(documents):\n",
      "    '''\n",
      "    Build an inverted index for a collection of documents: for each term in the \n",
      "    vocabulary, a list of the documents containing that term.\n",
      "    '''\n",
      "    index = defaultdict(list)\n",
      "    lexicon = build_lexicon(documents)\n",
      "    for term in lexicon:\n",
      "        index[term] = []\n",
      "        for d in documents:\n",
      "            if term in d.split():\n",
      "                index[term].append(documents.index(d))\n",
      "    return index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our collection of documents will consist of a few statements about animals. The important thing to note here is that the documents are similar, but none are identical. Our goal in this notebook is to see how we can query against this collection and still get useful results even when we get back multiple documents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#documents = ['peas porridge hot', 'peas porridge cold', 'peas porridge in the pot', 'nine days old']\n",
      "documents = ['The dog and cat ran very very fast', 'The cat sat very still', \n",
      "             'The gazelle ran fast', 'very cat', 'The lion was still', \n",
      "             'very', 'The dog ate the gazelle']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lexicon = build_lexicon(documents)\n",
      "index = build_inverted_index(documents)\n",
      "\n",
      "# This will be numpy.ndarray if we called sort instead of sorted above.\n",
      "print(type(lexicon))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'numpy.ndarray'>\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's the new stuff. We're going to build a matrix $A$, whose entry $a_{ij}$ contains a 1 if and only if term $i$ appears in document $j$. Otherwise, $a_{ij}$ is set to zero. So rows correspond to terms, and columns correspond to documents. In particular, we will think of the column $A[:,j]$ as a \"fairly good\" representation of document $j$.\n",
      "\n",
      "Some things to note: \n",
      "\n",
      "1. The number of rows is equal to the number of words in our lexicon. Right now we're dealing with small corpora, but lexicons can easily grow to be 30,000 words or larger. If the number of documents is large, the term-document matrix will be _huge_.\n",
      "2. Although large, the term-document matrix is also _sparse:_ almost all of the elements in the matrix are zero. In general, we will call an $n \\times n$ matrix sparse if the number of nonzero entries is $O(n)$. Often, the number of nonzero entries will be much smaller.\n",
      "\n",
      "Right now we're just focusing on document representation, but momentarily we will care about query representation as well. This is easy: we represent queries in _exactly_ the same way we represent documents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_termdoc_matrix(documents, lexicon=None):\n",
      "    '''\n",
      "    Build a simple binary term-document matrix.\n",
      "    '''\n",
      "    if lexicon == None:\n",
      "        lexicon = build_lexicon(documents)\n",
      "    nrows = len(lexicon)\n",
      "    ncols = len(documents)\n",
      "\n",
      "    termdoc_matrix = np.zeros((nrows, ncols))\n",
      "\n",
      "    for term in lexicon:\n",
      "        for d in documents:\n",
      "            term_idx = list(lexicon).index(term)\n",
      "            doc_idx = documents.index(d)\n",
      "            if term in d:\n",
      "                termdoc_matrix[term_idx, doc_idx] = 1\n",
      "    return termdoc_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "td_matrix = build_termdoc_matrix(documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we're representing our documents and queries as vectors, we want a way to say how \"close\" two vectors are. This will then tell us how close two documents are, or how close a query is to a document. Our solution to this problem will be to define the distance between two documents as the cosine of the angle between their vectors. This is called _cosine similarity_.\n",
      "\n",
      "If we have vectors $v_1$ and $v_2$ and the angle between them is $\\theta$, we'll use\n",
      "\n",
      "$$\\cos(\\theta) = \\frac{ v_1 \\cdot v_2 } {||v_1|| \\cdot ||v_2||}$$\n",
      "\n",
      "as our distance measure. This measure has some nice features: it works quickly for vectors in high dimensions, it's bounded (always between -1 and 1), and in our particular case it ranges from 0 (for very different vectors) to 1 (for identical vectors)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def distance(doc1, doc2):\n",
      "    '''\n",
      "    Returns the cosine distance between doc1 and doc2.\n",
      "    '''\n",
      "    return np.dot(doc1, doc2) / (np.linalg.norm(doc1) * np.linalg.norm(doc2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `build_vector` function below takes a query and a lexicon and constructs the vector representation of the query based on the given lexicon."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_vector(query, lexicon):\n",
      "    vec = np.zeros(len(lexicon))\n",
      "    parts = query.split()\n",
      "    for part in parts:\n",
      "        vec[np.where(lexicon == part)] = 1\n",
      "    return vec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = build_vector('cat', lexicon)\n",
      "print(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `get_postings` function returns all the documents that match the query."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_postings(query, docs, index = None):\n",
      "    '''\n",
      "    Return a set containing indices of all documents that match the \n",
      "    search query, optionally constructing an inverted index if none \n",
      "    is provided.\n",
      "    '''\n",
      "    postings = []\n",
      "    query_parts = query.split()\n",
      "    if query_parts == []:\n",
      "        return set()\n",
      "    if index == None:\n",
      "        index = build_inverted_index(docs)\n",
      "    individual_postings = [index[part] for part in query_parts]\n",
      "    return set.intersection(*map(set, individual_postings))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 226
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, our `search` function takes a query, a lexicon, an index, and a term-document matrix and returns the list of document ids that match the query, sorted in decreasing order by cosine distance from the query. \n",
      "\n",
      "There's a bit of Python-specific trickery in this function - make sure you understand what `sorted` is doing.\n",
      "\n",
      "Also note that we're printing the distances, which is not necessary in general and not recommended for large document collections."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def search(query, lexicon, index, tdmat):\n",
      "    '''\n",
      "    Perform a ranked search for the given query using the \n",
      "    given index and term-document matrix.\n",
      "    '''\n",
      "    vec = build_vector(query, lexicon)\n",
      "    candidates = get_hits(query, documents, index)\n",
      "    for c in candidates:\n",
      "        print distance(vec, tdmat[:, c])\n",
      "    results = sorted(list(candidates), key=lambda w: distance(vec, tdmat[:, w]), reverse=True)\n",
      "    return results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 227
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search('cat very', lexicon, index, td_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.534522483825\n",
        "0.632455532034\n",
        "1.0\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 228,
       "text": [
        "[3, 1, 0]"
       ]
      }
     ],
     "prompt_number": 228
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This function just assists in printing a human-readable representation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_docs(idx_list, docs):\n",
      "    '''\n",
      "    Given a list of documents and indices, return the documents corresponding \n",
      "    to the given indices. Useful for printing.\n",
      "    '''\n",
      "    ds = []\n",
      "    for idx in idx_list:\n",
      "        ds.append(docs[idx])\n",
      "    return ds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 229
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(get_docs(search('cat very', lexicon, index, td_matrix), documents))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.534522483825\n",
        "0.632455532034\n",
        "1.0\n",
        "['very cat', 'The cat sat very still', 'The dog and cat ran very very fast']\n"
       ]
      }
     ],
     "prompt_number": 230
    }
   ],
   "metadata": {}
  }
 ]
}